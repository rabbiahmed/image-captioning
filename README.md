# Image Captioning with CNN-RNN

**Generate descriptive captions for images using a deep learning model combining Computer Vision and Natural Language Processing.**

[![Python 3.x](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Setup and Installation](#setup-and-installation)
  - [Prerequisites](#prerequisites)
  - [Installation Steps](#installation-steps)
- [Usage](#usage)
  - [1. Data Preprocessing](#1-data-preprocessing)
  - [2. Model Training](#2-model-training)
  - [3. Model Inference](#3-model-inference)
  - [4. Data Exploration (Optional)](#4-data-exploration-optional)
- [Google Colab Usage](#google-colab-usage)
- [Model Architecture](#model-architecture)
- [Training Details](#training-details)
- [Results](#results)
- [Future Work](#future-work)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## Introduction

This project builds a deep learning model capable of understanding the content of an image and generating a human-like textual description.

The core idea is to use a pre-trained CNN (ResNet-50) to extract meaningful features from an image (encoding), and then feed these features into an RNN (LSTM) that learns to generate a sequence of words (decoding) forming a coherent caption.

## Features

* **CNN Encoder:** Utilizes a pre-trained ResNet-50 model to extract rich image features.
* **RNN Decoder:** Employs an LSTM (Long Short-Term Memory) network to generate sequential word predictions.
* **Vocabulary Management:** Builds a vocabulary from the training captions, handling unknown words and padding.
* **Dynamic Data Loading:** Downloads COCO images on-the-fly during training, reducing initial storage requirements (though local download is recommended for large-scale training).
* **Training Script:** Provides a script for training the CNN-RNN model with customizable hyperparameters.
* **Inference Script:** Allows generation of captions for new images using the trained model.
* **Google Colab Compatibility:** Designed to be runnable in Google Colab environments.

## Dataset

This project uses the **Microsoft COCO (Common Objects in Context) dataset**, specifically the 2017 split for training and validation.

* **COCO Annotations:** JSON files containing annotations for image instances, captions, and other tasks.
* **COCO Images:** The actual images referenced by the annotations.

The `preprocess_data.py` script and `train_model.py` are set up to fetch images directly from COCO URLs. However, for efficient and robust training, it is highly recommended to download the COCO 2017 images (`train2017` and `val2017`) locally.

**Download the annotations data (must)**:
* [2017 Train/Val annotations [241MB]](http://images.cocodataset.org/annotations/annotations_trainval2017.zip)

**Download mini COCO Dataset (Recommended):**
You can download the image data from the official COCO website:
* [2017 Train images [118K/18GB]](http://images.cocodataset.org/zips/train2017.zip)
* [2017 Val images [5K/1GB]](http://images.cocodataset.org/zips/val2017.zip)


After downloading, extract the image zips into an `images` folder and the annotations zip into an `annotations` folder within your project's root directory, so your structure looks like this:

## Project Structure

ImageCaptioningCOCO/
├── annotations/            # Stores COCO annotation JSON files (e.g., captions_train2017.json)
├── images/                 # (Optional, recommended) Stores downloaded COCO image data (e.g., train2017/, val2017/)
├── cnn_rnn_checkpoints/    # Stores trained model weights (encoder & decoder .pth files)
├── test_images/            # Directory to place sample images for inference
├── preprocess_data.py      # Handles data loading, image transformations, and vocabulary building
├── train_model.py          # Script for training the CNN-RNN model
├── inference_model.py      # Script for generating captions on new images
├── load_analyze_data.py    # Script to load and visualize sample COCO data
├── vocab.pkl               # Stores the vocabulary (word-to-index mapping), generated by train_model.py
├── .gitignore              # Specifies files/directories to be ignored by Git (e.g., generated models, venv)
├── README.md               # Project documentation
└── requirements.txt        # List of required Python packages

## Setup and Installation

### Prerequisites

* Python 3.8+
* `pip` (Python package installer)

### Installation Steps

1.  **Clone the repository:**
    ```bash
    git clone [Repository URL]
    cd ImageCaptioningCOCO
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    # On Windows:
    .\.venv\Scripts\activate
    # On macOS/Linux:
    source ./.venv/bin/activate
    ```
    
3.  **Install required Python packages:**
    ```bash
    pip install -r requirements.txt
    ```
    Your `requirements.txt` should contain (you can create this file if it doesn't exist):
    ```
    torch
    torchvision
    numpy
    scikit-image
    matplotlib
    tqdm
    pycocotools
    nltk
    Pillow
    ```
    *Note: `pycocotools` can sometimes be tricky to install on Windows. If you face issues, try `pip install pycocotools-windows` or refer to specific installation guides online for your OS.*


4.  **Download NLTK Data:**
    The `nltk.word_tokenize` function requires specific data packages (`punkt` and `punkt_tab`). Run these commands in your activated virtual environment:
    ```python
    import nltk
    nltk.download('punkt')
    nltk.download('punkt_tab')
    ```

5.  **Prepare COCO Dataset:**
    As mentioned in the [Dataset](#dataset) section, download the COCO 2017 `train2017` and `val2017` image data and the `annotations` zip.** Extract them into the `images` and `annotations` folders, respectively, in your project root. If you choose not to download images locally, the scripts will attempt to download them on-the-fly from COCO URLs, which is slower and prone to network errors.


6.  **Create `.gitignore` (Recommended):**
    To prevent large generated files from being tracked by Git, create a file named `.gitignore` in your project's root directory with the following content:
    ```
    .venv/
    __pycache__/
    *.pyc
    vocab.pkl
    cnn_rnn_checkpoints/
    test_images/
    ```

## Usage

Follow these steps to preprocess data, train the model, and generate captions.

### 1. Data Preprocessing

The `preprocess_data.py` script is primarily used internally by `train_model.py` to set up the data loaders and vocabulary. You can run it separately for testing and to confirm your data setup.

```bash
python preprocess_data.py
```
This script will:

- Load COCO annotations.
- Build a vocabulary from captions.
- Demonstrate loading a few batches of images and captions.

### 2. Model Training
The train_model.py script trains the CNN-RNN model. It will save the trained encoder and decoder weights, along with the vocabulary, for later inference.
```bash
python train_model.py
```
- Training Progress: You will see a tqdm progress bar showing the training progress for each epoch. Loss values will be printed periodically.

- Model Checkpoints: Trained encoder-X.pth and decoder-X.pth files (where X is the epoch number) will be saved in the cnn_rnn_checkpoints/ directory.

- Vocabulary: A vocab.pkl file will be saved in the root directory, containing the word-to-index mappings. This is crucial for inference.

- Hyperparameters: You can adjust hyperparameters like BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, EMBED_SIZE, and HIDDEN_SIZE within train_model.py.

### 3. Model Inference
The inference_model.py script uses a trained model to generate a caption for a new image.

- Place a test image: Put a sample image (e.g., my_image.jpg) into the test_images/ directory.
- Specify epoch: Open inference_model.py and set EPOCH_TO_LOAD to the epoch number of the model weights you wish to use (e.g., 5 if you trained for 5 epochs and want the last one).
- Specify image name: Update SAMPLE_IMAGE_NAME to the name of your test image.

Run the script:
```bash
python inference_model.py
```
The script will print the generated caption for your image.

### 4. Data Exploration (Optional)
The load_analyze_data.py script allows you to visualize a random image from the COCO dataset along with its associated captions. This is useful for understanding the dataset.
```bash
python load_analyze_data.py
```
### Google Colab Usage
This project is compatible with Google Colab. Follow these steps to set it up:

1. Create a new Colab Notebook.

2. Upload your project files:
- Click the folder icon on the left sidebar (Files).
- Click the "Upload to session storage" icon (folder with an arrow pointing up). Upload preprocess_data.py, train_model.py, inference_model.py, load_analyze_data.py, requirements.txt.
= Upload your annotations directory (containing captions_train2017.json, etc.) and optionally your images directory (if you downloaded them locally) to /content/.

Alternatively, you can mount Google Drive:
```bash
from google.colab import drive
drive.mount('/content/drive')
```
Then, adjust paths in your scripts to /content/drive/MyDrive/YourProjectFolder/

### 3. Install dependencies:
```bash
!pip install -r requirements.txt
```
### 4. Download NLTK Data:
```bash
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
```
### 5. Run the scripts:
Use the %run magic command to execute your Python scripts:
```bash
# For data preprocessing test
%run preprocess_data.py

# For training (ensure annotations and images are accessible)
%run train_model.py

# For inference (ensure model weights and vocab.pkl are generated/uploaded, and test_images folder exists)
%run inference_model.py
```
### Model Architecture
The model consists of two main components:

#### Encoder (CNN): 
A pre-trained ResNet-50 model (from torchvision.models) is used. The final classification layer is removed, and a new fully connected layer maps the extracted image features to a fixed embed_size. These features are then used to initialize the hidden and cell states of the LSTM.

- **Pre-training**: ResNet-50 is initialized with weights pre-trained on ImageNet.
- **Fine-tuning**: Only the new linear layer and Batch Normalization layer of the encoder are trained by default; the deeper ResNet layers are frozen initially for stability and faster training.

#### Decoder (RNN):
A Long Short-Term Memory (LSTM) network.

- **Embedding Layer**: Maps input word indices to dense word embeddings of embed_size.
- **LSTM Layer**: Processes the sequence of word embeddings, taking the image features as its initial hidden and cell states.
- **Linear Layer**: Maps the LSTM's output at each time step to the vocabulary size, generating logits for the next word prediction.
- **Greedy Sampling**: For inference, a greedy approach is used where the word with the highest probability is chosen at each step.

### Training Details
- **Loss Function**: nn.CrossEntropyLoss is used, with ignore_index=0 to ignore predictions for the <pad> token.
- **Optimizer**: Adam optimizer is used to update model parameters.
- **Data Loading**: Images are transformed (resized, cropped, normalized) and captions are tokenized, converted to numerical indices, and padded to a fixed length.
- **GPU Utilization**: The training script automatically detects and utilizes a CUDA-enabled GPU if available. If not, it gracefully falls back to CPU.

### Results
Work in progress.

### Future Work
- **Beam Search**: Implement beam search for better caption generation during inference, which explores multiple high-probability sequences instead of just the greedy best.
- **Attention Mechanism**: Integrate an attention mechanism into the decoder to allow the model to focus on relevant parts of the image when generating each word.
- **Hyperparameter Tuning**: Systematically tune hyperparameters (learning rate, hidden size, embedding size, dropout, number of LSTM layers, etc.) for optimal performance.
- **Evaluation Metrics**: Implement objective evaluation metrics (BLEU, CIDEr, etc.) to quantitatively assess model performance.
- **Pre-trained Embeddings**: Experiment with pre-trained word embeddings (e.g., Word2Vec, GloVe) instead of learning them from scratch.
- **Larger/Different CNN Backbones**: Experiment with more powerful CNN architectures (e.g., ResNeXt, EfficientNet).

### Contributing
Feel free to fork this repository, open issues, and submit pull requests.

### License
This project is licensed under the MIT License. See the LICENSE file for details.

### Acknowledgments
- The PyTorch team for the deep learning framework.
- The COCO dataset creators for providing a rich dataset for computer vision research.
- The NLTK library for natural language processing tools.
- The pycocotools library for COCO dataset API.
- Udacity for the Advanced Computer Vision and Deep Learning course.